\subsection[Classic Caches Improvements]{Classic Caches Improvements\footnote{by Nikos Nikoleris}}

The classic memory system implements a snooping MOESI-like coherence protocol that allows for flexible, configurable cache hierarchies.
The coherence protocol is primarily implemented in the Cache and the CoherentXBar and the SnoopFilter implements a common optimisation to reduce unnecessary coherence traffic.

Over the years, the components of the classic memory system have received a lot of contributions with a primary focus is adding support for future technologies and enhancing its accuracy.

\subsubsection[Non-Coherent Cache]{Non-Coherent Cache}
The cache model in gem5 implements the full coherence protocol and as a result can be used in any level of the coherent memory subsystem (e.g., L1 data cache or instruction cache, last-level cache).
The non-coherent cache is a stripped down version of the cache design to be used below the point-of-coherence (closer to memory).
Below the point-of-coherence, the non-coherent cache receives only requests for fetches and writebacks and itself send requests for fetches and writebacks to memory below. As opposed to the regular cache, the non-coherent cache will not send any snoops to invalidate or fetch data from caches above.
As such the non-coherent cache is a greatly simplified version in terms of handling the coherence protocol compared to the regular cache while otherwise supporting the same flexibility (e.g., configurable tags, replacement policies, inclusive or exclusive, etc.).

The non-coherent cache can model large system-level caches which are used by the CPUs and other devices in the system.

\subsubsection[Write Streaming Optimizations]{Write Streaming Optimizations}

Write streaming is a common access pattern which is typically encountered when software initialises or copies large memory buffers (e.g., memset, memcpy).
When executed, the core issues a large number of write requests to the data cache. The data cache receives these write requests and issues requests for exclusive copies of the corresponding cache lines. To get an exclusive copy, it has to invalidate copies of that line and fetch a copy of the data (e.g., from off-chip memory). As soon as it receives data, it performs all writes for that line and often will overwrite it completely. As a result, the data cache unnecessarily fetches data only to overwrite it shortly after. Often these write buffers are large in size and also trash the data cache.

Common optimizations~\cite{10.1145/173682.165154} coalesce writes to form full cache line writes, avoid unnecessary data fetches and achieve significant reduction in memory bandwidth.
In addition, when the written memory buffer is large, we can also avoid thrashing the data cache by bypassing allocation.

We have implemented a simple mechanism to detect write streaming access patterns and enable coalescing and bypassing.
The mechanism attaches to the data cache and analyses incoming write requests. When the number of sequential writes reaches a first threshold, it enables write coalescing and when a second threshold is reached, in addition, the cache will bypass allocation for the writes in the stream.

\subsubsection[Cache Maintenance Operations]{Cache Maintenance Operations}

Typically, the contents of the cache are handled by the coherence protocol.
For most user-level code, caches are invisible.
This greatly simplifies programming and ensures software portability.
However, when interfacing with devices or persistent memory, the effect of caching becomes visible to the programmer.
In such cases, a user might have to trigger a writeback which propagates all the way to the device or the persistent memory.
In other cases, a cache invalidation will ensure that a subsequent load will fetch the newest version of the data from a buffer of the main memory.

Cache maintenance operations (CMOs) are now supported in gem5 in a way that can deal with arbitrary cache hierarchies.
An operation can either clean and/or invalidate a cache line.
A clean
operation will find the dirty copy and trigger a writeback and an invalidate operation will find all copies of the cache line and invalidate them and the combined operation will perform both actions.
The effects of CMOs are defined with reference to a configurable point in the system.
For example, a clean and invalidate to the point-of-coherence will find all copies of the block above the point-of-coherence, invalidate them, and if any of them is dirty also trigger a writeback to the memory below the point-of-coherence.

\subsubsection[Snooping Support and Snoop Filtering]{Snooping Support and Snoop Filtering}

In large systems, broadcasting snoop messages is slow; costs energy and time; and can cause significant scalability bottlenecks.
Therefore, directories and filters are used to keep track of which caches / nodes are keeping a copy of a particular cached line.
We added a snoop filter to gem5 which is a distributed component that keeps track of the coherence state of all lines cached “above” it, similar to the AMD Probe Filter~\cite{} (Conway, Pat, Nathan Kalyanasundharam).
For example, if the snoop filter sits next to (in front of) the L3 cache, it knows about all lines in the L2 and L1 caches that are connected to that L3 cache.

Using the snoop filter, we can reduce the amount of messages from $O(N^2)$ to $O(N)$ with $N$ concurrent masters in the system.
Modelling the snoop filter / directory separately from the cache allows us to use different organizations for the filter and the cache, and distributing area between shared caches vs coherence tracking filters / directories.
We also model the effect of limited filter capacity through back-invalidations that remove cache entries if the filter becomes full; allowing for more realistic cache performance metrics.
Finally, the more centralized coherence tracking in the filter allows for more tight checking of correct function of the distributed coherence protocol in the classic memory system.
